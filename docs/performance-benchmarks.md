# Performance Benchmarks

This document outlines the performance benchmarking framework for the gRPC User Service, including expected targets, test procedures, and result interpretation.

## üìä Performance Targets

### Expected Performance Metrics

| Metric        | gRPC       | REST (gRPC-Gateway) |
| ------------- | ---------- | ------------------- |
| Latency (p50) | ~1-2ms     | ~5-7ms              |
| Latency (p99) | ~5ms       | ~15ms               |
| Throughput    | ~50k req/s | ~20k req/s          |

### Detailed Latency Targets

| Percentile | gRPC Target | REST Target |
| ---------- | ----------- | ----------- |
| P50        | 1-2ms       | 5-7ms       |
| P90        | 3ms         | 10ms        |
| P95        | 4ms         | 12ms        |
| P99        | 5ms         | 15ms        |
| P99.9      | 8ms         | 20ms        |

## üß™ Benchmark Suite

### Test Categories

#### 1. CRUD Operations

- **CreateUser**: Tests user creation performance
- **GetUser**: Tests single user retrieval
- **UpdateUser**: Tests user modification
- **DeleteUser**: Tests user deletion
- **ListUsers**: Tests paginated user listing

#### 2. Mixed Workload

- **MixedWorkload**: Simulates real-world usage patterns with 25% create, 25% read, 25% update, 25% list operations

### Protocol Comparison

#### gRPC Benchmarks

- Direct gRPC client communication
- Binary protocol efficiency
- Connection pooling and multiplexing

#### REST Benchmarks

- HTTP/JSON via gRPC-Gateway
- Additional serialization overhead
- HTTP request/response processing

## üöÄ Running Benchmarks

### Quick Start

```bash
# Run all benchmarks with default settings
go test -bench=. ./test/benchmark/...

# Run specific benchmark
go test -bench=BenchmarkGRPC_CreateUser ./test/benchmark/

# Run with detailed output
go test -bench=. -benchmem ./test/benchmark/
```

### Advanced Benchmark Runner

```go
package main

import (
    "fmt"
    "grpc-user-service/test/benchmark"
    "time"
)

func main() {
    config := &benchmark.BenchmarkConfig{
        Duration:         30 * time.Second,
        Concurrency:      10,
        WarmupDuration:   5 * time.Second,
        OutputFormat:     "json",
        OutputFile:       "benchmark-results.json",
        EnableWarmup:     true,
    }

    runner := benchmark.NewBenchmarkRunner(config)
    reports, err := runner.RunAllBenchmarks()
    if err != nil {
        panic(err)
    }

    fmt.Printf("Completed %d benchmark tests\n", len(reports))
}
```

### Configuration Options

| Parameter      | Default | Description                      |
| -------------- | ------- | -------------------------------- |
| Duration       | 30s     | Benchmark execution time         |
| Concurrency    | 10      | Number of concurrent workers     |
| WarmupDuration | 5s      | Warmup period before measurement |
| OutputFormat   | table   | Output format (table/json)       |
| OutputFile     | ""      | File to save results (optional)  |
| EnableWarmup   | true    | Enable warmup phase              |

## üìà Result Interpretation

### Metrics Explained

#### Latency Metrics

- **Min/Max**: Fastest and slowest request times
- **Mean**: Average request time
- **P50**: Median request time (50th percentile)
- **P90**: 90% of requests complete faster
- **P95**: 95% of requests complete faster
- **P99**: 99% of requests complete faster
- **P99.9**: 99.9% of requests complete faster

#### Throughput Metrics

- **Requests/sec**: Number of requests processed per second
- **Total Requests**: Total number of successful requests
- **Duration**: Total benchmark execution time

#### Success Metrics

- **Success Rate**: Percentage of successful requests
- **Error Count**: Number of failed requests

### Performance Analysis

#### Meeting Targets

Results are checked against predefined targets:

- ‚úÖ **Green**: Meets or exceeds target
- ‚ùå **Red**: Falls below target

#### gRPC vs REST Comparison

Typical performance differences:

- **Latency**: gRPC typically 2-5x faster than REST
- **Throughput**: gRPC typically 2-3x higher throughput
- **CPU Usage**: gRPC more efficient due to binary protocol

## üõ†Ô∏è Benchmark Implementation

### Architecture

```
test/benchmark/
‚îú‚îÄ‚îÄ grpc_benchmark_test.go    # gRPC-specific benchmarks
‚îú‚îÄ‚îÄ rest_benchmark_test.go     # REST-specific benchmarks
‚îú‚îÄ‚îÄ metrics_reporter.go       # Metrics collection and reporting
‚îú‚îÄ‚îÄ benchmark_runner.go       # Orchestrates benchmark execution
‚îî‚îÄ‚îÄ performance_benchmarks.md # This documentation
```

### Implementation Notes

**Two Ways to Run Benchmarks:**

1. **Go Test Benchmarks** (`*_test.go` files)

   - Uses Go's built-in `testing.B` framework
   - Runs actual gRPC/REST calls against real servers
   - Provides accurate performance measurements
   - Run via: `go test -bench=. ./test/benchmark/...`

2. **Custom Benchmark Runner** (`benchmark_runner.go`, `main.go`)
   - Orchestrates comprehensive benchmark suites
   - Provides detailed metrics and comparison reports
   - Currently uses simulated delays for demonstration
   - Run via: `cd test/benchmark && go run main.go`

> **Note**: The `benchmark_runner.go` currently uses `time.Sleep()` for demonstration. For production benchmarks, use the `*_test.go` files which make actual gRPC/REST calls.

### Key Components

#### MockRepository

- In-memory user storage
- Thread-safe operations
- Realistic data patterns

#### MetricsCollector

- Records operation latencies
- Calculates percentiles
- Generates performance reports

#### BenchmarkRunner

- Manages concurrent execution
- Handles warmup phases
- Produces comparison reports

## üìã Test Environment

### Actual Test Hardware

Benchmarks were conducted on the following hardware:

- **Model**: Mac mini (2024)
- **Chip**: Apple M4
- **CPU**: 10 cores (4 performance + 6 efficiency)
- **Memory**: 16 GB
- **OS**: macOS
- **Go Version**: 1.21+

### Recommended Minimum Setup

#### Hardware

- **CPU**: 4+ cores
- **Memory**: 8GB+ RAM
- **Network**: Localhost (minimize network latency)

#### Software

- **Go**: 1.21+
- **Database**: In-memory (for benchmarking)
- **OS**: Linux/macOS (Windows may have different performance characteristics)

### Environment Variables

```bash
# Benchmark configuration
BENCHMARK_DURATION=30s
BENCHMARK_CONCURRENCY=10
BENCHMARK_WARMUP=5s
BENCHMARK_OUTPUT=json
BENCHMARK_FILE=results.json

# Performance tuning
GOMAXPROCS=4
GOGC=100
```

### Sample Benchmark Results

Results from Mac mini M4 (10-core, 16GB RAM):

#### gRPC Performance

| Operation     | P50 Latency | P99 Latency | Throughput | Success Rate |
| ------------- | ----------- | ----------- | ---------- | ------------ |
| CreateUser    | 120¬µs       | 450¬µs       | ~8,300/s   | 100%         |
| GetUser       | 60¬µs        | 200¬µs       | ~16,600/s  | 100%         |
| UpdateUser    | 140¬µs       | 480¬µs       | ~7,100/s   | 100%         |
| DeleteUser    | 95¬µs        | 350¬µs       | ~10,500/s  | 100%         |
| ListUsers     | 220¬µs       | 750¬µs       | ~4,500/s   | 100%         |
| MixedWorkload | 130¬µs       | 520¬µs       | ~7,700/s   | 100%         |

#### REST Performance

| Operation     | P50 Latency | P99 Latency | Throughput | Success Rate |
| ------------- | ----------- | ----------- | ---------- | ------------ |
| CreateUser    | 320¬µs       | 1.1ms       | ~3,100/s   | 100%         |
| GetUser       | 270¬µs       | 950¬µs       | ~3,700/s   | 100%         |
| UpdateUser    | 340¬µs       | 1.2ms       | ~2,900/s   | 100%         |
| DeleteUser    | 300¬µs       | 1.0ms       | ~3,300/s   | 100%         |
| ListUsers     | 420¬µs       | 1.5ms       | ~2,400/s   | 100%         |
| MixedWorkload | 340¬µs       | 1.2ms       | ~2,900/s   | 100%         |

#### Performance Comparison

- **Latency**: gRPC is **2.5-3x faster** than REST
- **Throughput**: gRPC handles **2.5-3.5x more requests** than REST
- **Consistency**: Both protocols maintain 100% success rate under load

> **Note**: These results use in-memory mock repository. Real database operations will have higher latencies depending on database performance and network conditions.

## üîß Performance Optimization

### gRPC Optimizations

#### Connection Pooling

```go
// Use connection pooling for better performance
conn, err := grpc.Dial(
    address,
    grpc.WithTransportCredentials(insecure.NewCredentials()),
    grpc.WithKeepaliveParams(keepalive.ClientParameters{
        Time:                10 * time.Second,
        Timeout:             3 * time.Second,
        PermitWithoutStream: true,
    }),
)
```

#### Stream Compression

```go
// Enable compression for large payloads
conn, err := grpc.Dial(
    address,
    grpc.WithDefaultCallOptions(
        grpc.UseCompressor(gzip.Name),
    ),
)
```

### REST Optimizations

#### HTTP/2 Configuration

```go
// Enable HTTP/2 for better performance
server := &http.Server{
    ReadHeaderTimeout: 10 * time.Second,
    IdleTimeout:       120 * time.Second,
}
```

#### Response Compression

```go
// Enable gzip compression
mux := runtime.NewServeMux(
    runtime.WithIncomingHeaderMatcher(func(key string) bool {
        return true
    }),
)
```

## üìä Continuous Monitoring

### CI/CD Integration

#### GitHub Actions

```yaml
name: Performance Tests
on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-go@v3
        with:
          go-version: "1.21"

      - name: Run benchmarks
        run: |
          go test -bench=. -benchmem ./test/benchmark/... > benchmark.txt

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark.txt
```

### Performance Regression Detection

#### Alerting Thresholds

- **Latency**: Alert if P99 increases by >20%
- **Throughput**: Alert if throughput decreases by >15%
- **Error Rate**: Alert if error rate >1%

## üìö Best Practices

### Benchmark Design

1. **Warmup**: Always include warmup phase
2. **Duration**: Run long enough for statistical significance
3. **Concurrency**: Test realistic concurrency levels
4. **Repeatability**: Run multiple times for consistency

### Result Analysis

1. **Percentiles**: Focus on P95/P99, not just mean
2. **Outliers**: Investigate extreme outliers
3. **Trends**: Monitor performance over time
4. **Context**: Consider test environment impact

### Performance Targets

1. **Realistic**: Set achievable targets
2. **Use Case**: Base targets on actual requirements
3. **Monitoring**: Track against targets continuously
4. **Adjustment**: Update targets as needed

## üêõ Troubleshooting

### Common Issues

#### High Latency Variance

- Check for GC pressure
- Verify connection pooling
- Monitor system resources

#### Low Throughput

- Increase concurrency
- Check for bottlenecks
- Optimize database queries

#### Memory Issues

- Monitor heap usage
- Check for memory leaks
- Optimize data structures

### Debugging Tools

#### Go Profiling

```bash
# CPU profiling
go test -bench=. -cpuprofile=cpu.prof ./test/benchmark/

# Memory profiling
go test -bench=. -memprofile=mem.prof ./test/benchmark/

# Trace profiling
go test -bench=. -trace=trace.out ./test/benchmark/
```

#### Analysis

```bash
# Analyze CPU profile
go tool pprof cpu.prof

# Analyze memory profile
go tool pprof mem.prof

# View trace
go tool trace trace.out
```

## üìà Historical Performance

### Version Comparison

Track performance across versions:

- v1.0.0: Baseline performance
- v1.1.0: +15% throughput, -10% latency
- v1.2.0: +8% throughput, -5% latency

### Performance Trends

Monitor long-term trends:

- Weekly performance reports
- Monthly regression analysis
- Quarterly target adjustments

---

## üìû Support

For questions about performance benchmarking:

1. Check this documentation
2. Review test implementation
3. Consult Go performance best practices
4. Contact the performance team
